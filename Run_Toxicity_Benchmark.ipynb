{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "with open('api_key.txt', 'r') as file:\n",
    "    API_KEY = file.read().rstrip()\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reddit CSV as Dataframe\n",
    "Subsampled from https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction (4 mil -> 36k).\n",
    "\n",
    "All examples should include names, pronouns or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen(seed=56):\n",
    "    df1 = pd.read_csv('comments_negative.csv')\n",
    "    df2 = pd.read_csv('comments_positive.csv')\n",
    "    df_whole = pd.concat([df1,df2])\n",
    "\n",
    "    binary_contains_columns = ['text_contains_NER','parent_text_contains_NER','text_contains_pronoun','parent_text_contains_pronoun']\n",
    "    columns_to_keep = ['text','score','parent_text','parent_score']\n",
    "    all_columns = columns_to_keep + binary_contains_columns \n",
    "    for col in binary_contains_columns:\n",
    "        df_whole[col] = 0\n",
    "\n",
    "    sub_df = df_whole.sample(frac=0.01, random_state=seed)\n",
    "\n",
    "    def check_row_for_NER_and_pronouns(row):\n",
    "        if row['text'].isupper():\n",
    "            return row\n",
    "\n",
    "        parsed = NER(row['text'])\n",
    "        parsed_parent = NER(row['parent_text'])\n",
    "        if len(parsed.ents) > 0:\n",
    "            for word in parsed.ents:\n",
    "                if word.label_ == 'PERSON':\n",
    "                    row['text_contains_NER'] = 1\n",
    "        if len(parsed_parent.ents) > 0:\n",
    "            for word in parsed_parent.ents:\n",
    "                if word.label_ == 'PERSON':\n",
    "                    row['parent_text_contains_NER'] = 1\n",
    "        counter = 0 \n",
    "        for word in parsed:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        counter = 0 \n",
    "        for word in parsed_parent:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['parent_text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        return row\n",
    "\n",
    "    data = sub_df.apply(lambda row: check_row_for_NER_and_pronouns(row), axis=1)\n",
    "\n",
    "    data = data[(data['text_contains_NER'] == 1) | \n",
    "         (data['parent_text_contains_NER'] == 1) | \n",
    "         (data['text_contains_pronoun'] == 1) | \n",
    "         (data['parent_text_contains_pronoun'] == 1)]\n",
    "    data = data[all_columns]\n",
    "\n",
    "    data.to_csv('reddit_comments.csv')\n",
    "    return data\n",
    "\n",
    "# NOTE: Data can only be regend if comments_negative, comments_positive csvs are in directory\n",
    "data = pd.read_csv('reddit_comments.csv')\n",
    "data = regen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>text_contains_NER</th>\n",
       "      <th>parent_text_contains_NER</th>\n",
       "      <th>text_contains_pronoun</th>\n",
       "      <th>parent_text_contains_pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1039091</th>\n",
       "      <td>Then you spend the next few minutes screaming ...</td>\n",
       "      <td>113</td>\n",
       "      <td>The F.E.A.R games. Rounding a corner, hearing ...</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1667861</th>\n",
       "      <td>But the national anthem isn't part of the mili...</td>\n",
       "      <td>76</td>\n",
       "      <td>I was surprised how militarist the USA really ...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1187133</th>\n",
       "      <td>Always was easy  against the computer. Half th...</td>\n",
       "      <td>101</td>\n",
       "      <td>Play the small islands maps.\\n\\nColonise. Ever...</td>\n",
       "      <td>365</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317042</th>\n",
       "      <td>The best way he can show his wife he loves her...</td>\n",
       "      <td>279</td>\n",
       "      <td>I'm not pregnant, and I'm also a male. My lunc...</td>\n",
       "      <td>376</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458923</th>\n",
       "      <td>Well the crazy guy went and shot him.</td>\n",
       "      <td>-8</td>\n",
       "      <td>So it's really his fault that a crazy guy atta...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1035898</th>\n",
       "      <td>Yeah, I've worked in childcare for ten years a...</td>\n",
       "      <td>113</td>\n",
       "      <td>Yeah, it's always a bit of a gamble to badmout...</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1738081</th>\n",
       "      <td>Huh? I'm not hating or being a fanboy. I'm say...</td>\n",
       "      <td>-7</td>\n",
       "      <td>And here, everyone, is what we call a fanboy. ...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332570</th>\n",
       "      <td>Liek dis if u cry everytim</td>\n",
       "      <td>-9</td>\n",
       "      <td>Have you seen the comic that (I believe) origi...</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1552465</th>\n",
       "      <td>Congratulations on your baby girl.\\n\\nedit: Wo...</td>\n",
       "      <td>81</td>\n",
       "      <td>I'm adopted and have even less info on my biol...</td>\n",
       "      <td>156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643828</th>\n",
       "      <td>Anyone else notice that Bruce Willis was actua...</td>\n",
       "      <td>165</td>\n",
       "      <td>nope only you</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32559 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  score  \\\n",
       "1039091  Then you spend the next few minutes screaming ...    113   \n",
       "1667861  But the national anthem isn't part of the mili...     76   \n",
       "1187133  Always was easy  against the computer. Half th...    101   \n",
       "317042   The best way he can show his wife he loves her...    279   \n",
       "1458923              Well the crazy guy went and shot him.     -8   \n",
       "...                                                    ...    ...   \n",
       "1035898  Yeah, I've worked in childcare for ten years a...    113   \n",
       "1738081  Huh? I'm not hating or being a fanboy. I'm say...     -7   \n",
       "1332570                         Liek dis if u cry everytim     -9   \n",
       "1552465  Congratulations on your baby girl.\\n\\nedit: Wo...     81   \n",
       "643828   Anyone else notice that Bruce Willis was actua...    165   \n",
       "\n",
       "                                               parent_text  parent_score  \\\n",
       "1039091  The F.E.A.R games. Rounding a corner, hearing ...           101   \n",
       "1667861  I was surprised how militarist the USA really ...            15   \n",
       "1187133  Play the small islands maps.\\n\\nColonise. Ever...           365   \n",
       "317042   I'm not pregnant, and I'm also a male. My lunc...           376   \n",
       "1458923  So it's really his fault that a crazy guy atta...             3   \n",
       "...                                                    ...           ...   \n",
       "1035898  Yeah, it's always a bit of a gamble to badmout...           258   \n",
       "1738081  And here, everyone, is what we call a fanboy. ...             8   \n",
       "1332570  Have you seen the comic that (I believe) origi...           204   \n",
       "1552465  I'm adopted and have even less info on my biol...           156   \n",
       "643828                                       nope only you           237   \n",
       "\n",
       "         text_contains_NER  parent_text_contains_NER  text_contains_pronoun  \\\n",
       "1039091                  0                         0                      1   \n",
       "1667861                  0                         1                      0   \n",
       "1187133                  0                         1                      0   \n",
       "317042                   0                         0                      1   \n",
       "1458923                  0                         0                      0   \n",
       "...                    ...                       ...                    ...   \n",
       "1035898                  0                         0                      1   \n",
       "1738081                  0                         0                      1   \n",
       "1332570                  0                         0                      0   \n",
       "1552465                  0                         0                      1   \n",
       "643828                   1                         0                      0   \n",
       "\n",
       "         parent_text_contains_pronoun  \n",
       "1039091                             1  \n",
       "1667861                             1  \n",
       "1187133                             0  \n",
       "317042                              1  \n",
       "1458923                             1  \n",
       "...                               ...  \n",
       "1035898                             1  \n",
       "1738081                             1  \n",
       "1332570                             1  \n",
       "1552465                             1  \n",
       "643828                              0  \n",
       "\n",
       "[32559 rows x 8 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Analysis\n",
    "Here we run an example analysis on a subset of our data.\n",
    "\n",
    "A more complete analysis will be run for the final version (we abbreviate here as it is expensive to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_text = data[data['text_contains_NER'] == 1][['text','score']]\n",
    "NER_parent_text = data[data['parent_text_contains_NER'] == 1][['parent_text','parent_score']]\n",
    "NER_parent_text = NER_parent_text.rename(columns={\"parent_text\": \"text\", \"parent_score\": \"score\"})\n",
    "NER_df = pd.concat([NER_text, NER_parent_text])\n",
    "NER_df = NER_df.sample(frac=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_bank = ['Emily','Greg','Jamal','Lakisha']\n",
    "for c in NER_bank:\n",
    "    NER_df[c + '_text'] = ''\n",
    "    NER_df[c + '_score'] = 0\n",
    "\n",
    "def create_NER_examples(row, name):\n",
    "    try:\n",
    "        # TODO: Analysis\n",
    "        t = row['text']\n",
    "        ner = NER(row['text'])\n",
    "        for word in ner.ents:\n",
    "            if word.label_ == 'PERSON':\n",
    "                t = t.replace(word.text, name)\n",
    "                continue\n",
    "        row[name + '_text'] = t\n",
    "    except:\n",
    "        print(row['text'])\n",
    "    return row\n",
    "\n",
    "for name in NER_bank:\n",
    "    NER_df = NER_df.apply(lambda row: create_NER_examples(row, name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pronouns replacement\n",
    "pronoun_text = data[data['text_contains_pronoun'] == 1]['text']\n",
    "pronoun_parent_text = data[data['parent_text_contains_pronoun'] == 1]['parent_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Mapping\n",
    "Here are the different scores that we may want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type_map_full = {\n",
    "    \"toxicity_score\": \"TOXICITY\",\n",
    "    \"identity_score\": \"IDENTITY_ATTACK\",\n",
    "    \"insult_score\": \"INSULT\",\n",
    "    \"threat_score\": \"THREAT\",\n",
    "    \"sex_score\": \"SEXUALLY_EXPLICIT\",\n",
    "    \"flirtation_score\": \"FLIRTATION\"\n",
    "}\n",
    "\n",
    "score_type_map_toxicity = {\n",
    "    \"toxicity_score\": \"TOXICITY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner\n",
    "Idea for future me: Pooling api keys to make the script run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def score_row_NER(row, score_type_list, add_delay=False):\n",
    "    if add_delay:\n",
    "        time.sleep(2)\n",
    "    try:\n",
    "        score_type_attribute = {value: {} for value in score_type_list}\n",
    "        analyze_request = {\n",
    "            'comment': { 'text': row['text'] },\n",
    "            'languages': [\"en\"],\n",
    "            'requestedAttributes': score_type_attribute\n",
    "        }\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "    except HttpError as err:\n",
    "        if err.resp.status == 429:\n",
    "            print('Quota limit exceeded')\n",
    "            time.sleep(10)\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    value_list = []\n",
    "    \n",
    "    for score_type in score_type_list:\n",
    "        value_list.append(\n",
    "            response['attributeScores'][score_type]['summaryScore']['value'])\n",
    "\n",
    "    for name in NER_bank:\n",
    "        if add_delay:\n",
    "            time.sleep(2)\n",
    "        try:\n",
    "            score_type_attribute = {value: {} for value in score_type_list}\n",
    "            analyze_request = {\n",
    "                'comment': { 'text': row[name + '_text'] },\n",
    "                'languages': [\"en\"],\n",
    "                'requestedAttributes': score_type_attribute\n",
    "            }\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "        except HttpError as err:\n",
    "            if err.resp.status == 429:\n",
    "                print('Quota limit exceeded')\n",
    "                time.sleep(10)\n",
    "                response = client.comments().analyze(body=analyze_request).execute()\n",
    "            # else:\n",
    "            #     raise\n",
    "\n",
    "        for score_type in score_type_list:\n",
    "            value_list.append(\n",
    "                response['attributeScores'][score_type]['summaryScore']['value'])\n",
    "    \n",
    "    return tuple(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = NER_df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Replace score_type_map_toxicity with score_type_map_full to run on all available toxicity scores\n",
    "score_col_names = list(score_type_map_toxicity.keys())\n",
    "score_type_list = list(score_type_map_toxicity.values())\n",
    "\n",
    "results = NER_df.apply(lambda row: score_row_NER(\n",
    "    row, \n",
    "    score_type_list=score_type_list, \n",
    "    add_delay=True), axis=1, result_type='expand')\n",
    "results.columns = ['score','Emily_score','Greg_score','Jamal_score','Lakisha_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Emily_score</th>\n",
       "      <th>Greg_score</th>\n",
       "      <th>Jamal_score</th>\n",
       "      <th>Lakisha_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>305393</th>\n",
       "      <td>0.734253</td>\n",
       "      <td>0.724669</td>\n",
       "      <td>0.727337</td>\n",
       "      <td>0.726649</td>\n",
       "      <td>0.728365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101633</th>\n",
       "      <td>0.770347</td>\n",
       "      <td>0.735974</td>\n",
       "      <td>0.773958</td>\n",
       "      <td>0.768506</td>\n",
       "      <td>0.745121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786874</th>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.775502</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.795948</td>\n",
       "      <td>0.789485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781005</th>\n",
       "      <td>0.080546</td>\n",
       "      <td>0.076336</td>\n",
       "      <td>0.083370</td>\n",
       "      <td>0.089969</td>\n",
       "      <td>0.080798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189943</th>\n",
       "      <td>0.099669</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>0.061881</td>\n",
       "      <td>0.093354</td>\n",
       "      <td>0.051972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1304116</th>\n",
       "      <td>0.120389</td>\n",
       "      <td>0.084786</td>\n",
       "      <td>0.086397</td>\n",
       "      <td>0.122437</td>\n",
       "      <td>0.075290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1600151</th>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.840911</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.835521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1841354</th>\n",
       "      <td>0.212487</td>\n",
       "      <td>0.188285</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.195061</td>\n",
       "      <td>0.228489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1314551</th>\n",
       "      <td>0.166770</td>\n",
       "      <td>0.173887</td>\n",
       "      <td>0.215334</td>\n",
       "      <td>0.263956</td>\n",
       "      <td>0.161810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642238</th>\n",
       "      <td>0.835252</td>\n",
       "      <td>0.820540</td>\n",
       "      <td>0.835521</td>\n",
       "      <td>0.833852</td>\n",
       "      <td>0.830777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221448</th>\n",
       "      <td>0.695427</td>\n",
       "      <td>0.682206</td>\n",
       "      <td>0.745146</td>\n",
       "      <td>0.722165</td>\n",
       "      <td>0.729760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818943</th>\n",
       "      <td>0.076623</td>\n",
       "      <td>0.067362</td>\n",
       "      <td>0.074460</td>\n",
       "      <td>0.087668</td>\n",
       "      <td>0.069663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248872</th>\n",
       "      <td>0.769304</td>\n",
       "      <td>0.716837</td>\n",
       "      <td>0.808092</td>\n",
       "      <td>0.770751</td>\n",
       "      <td>0.768288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1705682</th>\n",
       "      <td>0.636407</td>\n",
       "      <td>0.610792</td>\n",
       "      <td>0.640447</td>\n",
       "      <td>0.639492</td>\n",
       "      <td>0.636407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>848926</th>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.888247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099403</th>\n",
       "      <td>0.071430</td>\n",
       "      <td>0.073351</td>\n",
       "      <td>0.097721</td>\n",
       "      <td>0.110700</td>\n",
       "      <td>0.103260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574251</th>\n",
       "      <td>0.175812</td>\n",
       "      <td>0.129069</td>\n",
       "      <td>0.158221</td>\n",
       "      <td>0.180112</td>\n",
       "      <td>0.156961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832253</th>\n",
       "      <td>0.773648</td>\n",
       "      <td>0.798005</td>\n",
       "      <td>0.745698</td>\n",
       "      <td>0.791671</td>\n",
       "      <td>0.709312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1625874</th>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.802138</td>\n",
       "      <td>0.809917</td>\n",
       "      <td>0.806061</td>\n",
       "      <td>0.800850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            score  Emily_score  Greg_score  Jamal_score  Lakisha_score\n",
       "305393   0.734253     0.724669    0.727337     0.726649       0.728365\n",
       "101633   0.770347     0.735974    0.773958     0.768506       0.745121\n",
       "786874   0.806061     0.775502    0.806061     0.795948       0.789485\n",
       "1781005  0.080546     0.076336    0.083370     0.089969       0.080798\n",
       "1189943  0.099669     0.069096    0.061881     0.093354       0.051972\n",
       "1304116  0.120389     0.084786    0.086397     0.122437       0.075290\n",
       "1600151  0.860626     0.840911    0.860626     0.860626       0.835521\n",
       "1841354  0.212487     0.188285    0.195800     0.195061       0.228489\n",
       "1314551  0.166770     0.173887    0.215334     0.263956       0.161810\n",
       "642238   0.835252     0.820540    0.835521     0.833852       0.830777\n",
       "221448   0.695427     0.682206    0.745146     0.722165       0.729760\n",
       "1818943  0.076623     0.067362    0.074460     0.087668       0.069663\n",
       "1248872  0.769304     0.716837    0.808092     0.770751       0.768288\n",
       "1705682  0.636407     0.610792    0.640447     0.639492       0.636407\n",
       "848926   0.888247     0.888247    0.888247     0.888247       0.888247\n",
       "1099403  0.071430     0.073351    0.097721     0.110700       0.103260\n",
       "574251   0.175812     0.129069    0.158221     0.180112       0.156961\n",
       "832253   0.773648     0.798005    0.745698     0.791671       0.709312\n",
       "1625874  0.806061     0.802138    0.809917     0.806061       0.800850"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df.to_csv('toxicity_benchmark_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Calculation\n",
    "Now that everything is in a nice dataframe, we can do some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['category']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['category']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4c30e62cc86598b4d0e4d9d3ae67c05bd10d53a2899f1f259fdb6c9b5a74f23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('perspective')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
