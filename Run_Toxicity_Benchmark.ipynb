{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "with open('api_key.txt', 'r') as file:\n",
    "    API_KEY = file.read().rstrip()\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "phrase_matcher = PhraseMatcher(NER.vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reddit CSV as Dataframe\n",
    "Subsampled from https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction (4 mil -> 36k).\n",
    "\n",
    "All examples should include names, pronouns or both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen(seed=56):\n",
    "    df1 = pd.read_csv('comments_negative.csv')\n",
    "    df2 = pd.read_csv('comments_positive.csv')\n",
    "    df_whole = pd.concat([df1,df2])\n",
    "\n",
    "    binary_contains_columns = ['text_contains_NER','parent_text_contains_NER','text_contains_pronoun','parent_text_contains_pronoun']\n",
    "    columns_to_keep = ['text','score','parent_text','parent_score']\n",
    "    all_columns = columns_to_keep + binary_contains_columns \n",
    "    for col in binary_contains_columns:\n",
    "        df_whole[col] = 0\n",
    "\n",
    "    sub_df = df_whole.sample(frac=0.01, random_state=seed)\n",
    "\n",
    "    def check_row_for_NER_and_pronouns(row):\n",
    "        if row['text'].isupper():\n",
    "            return row\n",
    "\n",
    "        parsed = NER(row['text'])\n",
    "        parsed_parent = NER(row['parent_text'])\n",
    "        if len(parsed.ents) > 0:\n",
    "            for word in parsed.ents:\n",
    "                if word.label_ == 'PERSON':\n",
    "                    row['text_contains_NER'] = 1\n",
    "        if len(parsed_parent.ents) > 0:\n",
    "            for word in parsed_parent.ents:\n",
    "                if word.label_ == 'PERSON':\n",
    "                    row['parent_text_contains_NER'] = 1\n",
    "        counter = 0 \n",
    "        for word in parsed:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        counter = 0 \n",
    "        for word in parsed_parent:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['parent_text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        return row\n",
    "\n",
    "    data = sub_df.apply(lambda row: check_row_for_NER_and_pronouns(row), axis=1)\n",
    "\n",
    "    data = data[(data['text_contains_NER'] == 1) | \n",
    "         (data['parent_text_contains_NER'] == 1) | \n",
    "         (data['text_contains_pronoun'] == 1) | \n",
    "         (data['parent_text_contains_pronoun'] == 1)]\n",
    "    data = data[all_columns]\n",
    "\n",
    "    data.to_csv('reddit_comments.csv')\n",
    "    return data\n",
    "\n",
    "# NOTE: Data can only be regend if comments_negative, comments_positive csvs are in directory\n",
    "data = pd.read_csv('reddit_comments.csv')\n",
    "# data = regen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>text_contains_NER</th>\n",
       "      <th>parent_text_contains_NER</th>\n",
       "      <th>text_contains_pronoun</th>\n",
       "      <th>parent_text_contains_pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039091</td>\n",
       "      <td>Then you spend the next few minutes screaming ...</td>\n",
       "      <td>113</td>\n",
       "      <td>The F.E.A.R games. Rounding a corner, hearing ...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1667861</td>\n",
       "      <td>But the national anthem isn't part of the mili...</td>\n",
       "      <td>76</td>\n",
       "      <td>I was surprised how militarist the USA really ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1187133</td>\n",
       "      <td>Always was easy  against the computer. Half th...</td>\n",
       "      <td>101</td>\n",
       "      <td>Play the small islands maps.\\n\\nColonise. Ever...</td>\n",
       "      <td>365.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317042</td>\n",
       "      <td>The best way he can show his wife he loves her...</td>\n",
       "      <td>279</td>\n",
       "      <td>I'm not pregnant, and I'm also a male. My lunc...</td>\n",
       "      <td>376.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1458923</td>\n",
       "      <td>Well the crazy guy went and shot him.</td>\n",
       "      <td>-8</td>\n",
       "      <td>So it's really his fault that a crazy guy atta...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32560</th>\n",
       "      <td>1035898</td>\n",
       "      <td>Yeah, I've worked in childcare for ten years a...</td>\n",
       "      <td>113</td>\n",
       "      <td>Yeah, it's always a bit of a gamble to badmout...</td>\n",
       "      <td>258.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32561</th>\n",
       "      <td>1738081</td>\n",
       "      <td>Huh? I'm not hating or being a fanboy. I'm say...</td>\n",
       "      <td>-7</td>\n",
       "      <td>And here, everyone, is what we call a fanboy. ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32562</th>\n",
       "      <td>1332570</td>\n",
       "      <td>Liek dis if u cry everytim</td>\n",
       "      <td>-9</td>\n",
       "      <td>Have you seen the comic that (I believe) origi...</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32563</th>\n",
       "      <td>1552465</td>\n",
       "      <td>Congratulations on your baby girl.\\n\\nedit: Wo...</td>\n",
       "      <td>81</td>\n",
       "      <td>I'm adopted and have even less info on my biol...</td>\n",
       "      <td>156.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32564</th>\n",
       "      <td>643828</td>\n",
       "      <td>Anyone else notice that Bruce Willis was actua...</td>\n",
       "      <td>165</td>\n",
       "      <td>nope only you</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32565 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text score  \\\n",
       "0        1039091  Then you spend the next few minutes screaming ...   113   \n",
       "1        1667861  But the national anthem isn't part of the mili...    76   \n",
       "2        1187133  Always was easy  against the computer. Half th...   101   \n",
       "3         317042  The best way he can show his wife he loves her...   279   \n",
       "4        1458923              Well the crazy guy went and shot him.    -8   \n",
       "...          ...                                                ...   ...   \n",
       "32560    1035898  Yeah, I've worked in childcare for ten years a...   113   \n",
       "32561    1738081  Huh? I'm not hating or being a fanboy. I'm say...    -7   \n",
       "32562    1332570                         Liek dis if u cry everytim    -9   \n",
       "32563    1552465  Congratulations on your baby girl.\\n\\nedit: Wo...    81   \n",
       "32564     643828  Anyone else notice that Bruce Willis was actua...   165   \n",
       "\n",
       "                                             parent_text  parent_score  \\\n",
       "0      The F.E.A.R games. Rounding a corner, hearing ...         101.0   \n",
       "1      I was surprised how militarist the USA really ...          15.0   \n",
       "2      Play the small islands maps.\\n\\nColonise. Ever...         365.0   \n",
       "3      I'm not pregnant, and I'm also a male. My lunc...         376.0   \n",
       "4      So it's really his fault that a crazy guy atta...           3.0   \n",
       "...                                                  ...           ...   \n",
       "32560  Yeah, it's always a bit of a gamble to badmout...         258.0   \n",
       "32561  And here, everyone, is what we call a fanboy. ...           8.0   \n",
       "32562  Have you seen the comic that (I believe) origi...         204.0   \n",
       "32563  I'm adopted and have even less info on my biol...         156.0   \n",
       "32564                                      nope only you         237.0   \n",
       "\n",
       "       text_contains_NER  parent_text_contains_NER  text_contains_pronoun  \\\n",
       "0                    0.0                       0.0                    1.0   \n",
       "1                    0.0                       1.0                    0.0   \n",
       "2                    0.0                       1.0                    0.0   \n",
       "3                    0.0                       0.0                    1.0   \n",
       "4                    0.0                       0.0                    0.0   \n",
       "...                  ...                       ...                    ...   \n",
       "32560                0.0                       0.0                    1.0   \n",
       "32561                0.0                       0.0                    1.0   \n",
       "32562                0.0                       0.0                    0.0   \n",
       "32563                0.0                       0.0                    1.0   \n",
       "32564                1.0                       0.0                    0.0   \n",
       "\n",
       "       parent_text_contains_pronoun  \n",
       "0                               1.0  \n",
       "1                               1.0  \n",
       "2                               0.0  \n",
       "3                               1.0  \n",
       "4                               1.0  \n",
       "...                             ...  \n",
       "32560                           1.0  \n",
       "32561                           1.0  \n",
       "32562                           1.0  \n",
       "32563                           1.0  \n",
       "32564                           0.0  \n",
       "\n",
       "[32565 rows x 9 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Analysis\n",
    "Here we run an example analysis on a subset of our data.\n",
    "\n",
    "A more complete analysis will be run for the final version (we abbreviate here as it is expensive to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_text = data[data['text_contains_NER'] == 1][['text','score']]\n",
    "NER_parent_text = data[data['parent_text_contains_NER'] == 1][['parent_text','parent_score']]\n",
    "NER_parent_text = NER_parent_text.rename(columns={\"parent_text\": \"text\", \"parent_score\": \"score\"})\n",
    "NER_df = pd.concat([NER_text, NER_parent_text])\n",
    "NER_df = NER_df.sample(frac=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_bank = ['Emily','Greg','Jamal','Lakisha']\n",
    "for c in NER_bank:\n",
    "    NER_df[c + '_text'] = ''\n",
    "    # NER_df[c + '_score'] = 0\n",
    "\n",
    "phrases = NER_bank\n",
    "patterns = [NER(n) for n in phrases]\n",
    "phrase_matcher.add('names', None, *patterns)\n",
    "\n",
    "def create_NER_examples(row, name):\n",
    "    try:\n",
    "        # TODO: Analysis\n",
    "        found = False\n",
    "        t = row['text']\n",
    "        ner = NER(row['text'])\n",
    "        for word in ner.ents:\n",
    "            if word.label_ == 'PERSON':\n",
    "                t = t.replace(word.text, name)\n",
    "                if not found:\n",
    "                    for sent in NER(t).sents:\n",
    "                        if not found:\n",
    "                            for match_id, start, end in phrase_matcher(NER(sent.text)):\n",
    "                                \n",
    "                                if NER.vocab.strings[match_id] in [\"names\"]:\n",
    "                                    row[name + '_text'] = sent.text\n",
    "                                    found = True\n",
    "                                    break\n",
    "                        else:\n",
    "                            break\n",
    "                    else:\n",
    "                        break\n",
    "                            \n",
    "    except:\n",
    "        print(row['text'])\n",
    "    return row\n",
    "\n",
    "for name in NER_bank:\n",
    "    NER_df = NER_df.apply(lambda row: create_NER_examples(row, name), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>Emily_text</th>\n",
       "      <th>Greg_text</th>\n",
       "      <th>Jamal_text</th>\n",
       "      <th>Lakisha_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6593</th>\n",
       "      <td>I'd say we are paying too little for games lik...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>I'd say we are paying too little for games lik...</td>\n",
       "      <td>I'd say we are paying too little for games lik...</td>\n",
       "      <td>I'd say we are paying too little for games lik...</td>\n",
       "      <td>I'd say we are paying too little for games lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21192</th>\n",
       "      <td>It's mother had been killed so he brought it u...</td>\n",
       "      <td>65.0</td>\n",
       "      <td>He calls it Emily and I call it Rikimaru.</td>\n",
       "      <td>He calls it Greg and I call it Rikimaru.</td>\n",
       "      <td>He calls it Jamal and I call it Rikimaru.</td>\n",
       "      <td>He calls it Lakisha and I call it Rikimaru.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4454</th>\n",
       "      <td>Halle Berry's tits, so everybody wins.</td>\n",
       "      <td>92.0</td>\n",
       "      <td>Emily tits, so everybody wins.</td>\n",
       "      <td>Greg tits, so everybody wins.</td>\n",
       "      <td>Jamal tits, so everybody wins.</td>\n",
       "      <td>Lakisha tits, so everybody wins.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22030</th>\n",
       "      <td>Jamie Foxx has kids too. So does Christoph Wal...</td>\n",
       "      <td>97</td>\n",
       "      <td>Emily has kids too.</td>\n",
       "      <td>Greg has kids too.</td>\n",
       "      <td>Jamal has kids too.</td>\n",
       "      <td>Lakisha has kids too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1486</th>\n",
       "      <td>Sort of an Ironic thing to say on reddit, no? ...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>People on here always advocating voting the ca...</td>\n",
       "      <td>People on here always advocating voting the ca...</td>\n",
       "      <td>People on here always advocating voting the ca...</td>\n",
       "      <td>People on here always advocating voting the ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>now your calling something that disagrees with...</td>\n",
       "      <td>-7</td>\n",
       "      <td>your calling something that disagrees with you...</td>\n",
       "      <td>your calling something that disagrees with you...</td>\n",
       "      <td>your calling something that disagrees with you...</td>\n",
       "      <td>your calling something that disagrees with you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28262</th>\n",
       "      <td>I've always found the Dutch to be easy to spot...</td>\n",
       "      <td>133</td>\n",
       "      <td>When in groups they are easy to spot because y...</td>\n",
       "      <td>When in groups they are easy to spot because y...</td>\n",
       "      <td>When in groups they are easy to spot because y...</td>\n",
       "      <td>When in groups they are easy to spot because y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7496</th>\n",
       "      <td>Fuck Ron Paul?\\n(Wonka Face)\\nPlease, tell me ...</td>\n",
       "      <td>-20</td>\n",
       "      <td>Fuck Emily?\\n</td>\n",
       "      <td>Fuck Greg?</td>\n",
       "      <td>Fuck Jamal?\\n</td>\n",
       "      <td>Fuck Lakisha?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14588</th>\n",
       "      <td>&amp;gt;*Everything sounds better in Latin.*\\n\\nEx...</td>\n",
       "      <td>196</td>\n",
       "      <td>*\\n\\nExcept Emily...</td>\n",
       "      <td>*\\n\\nExcept Greg...</td>\n",
       "      <td>*\\n\\nExcept Jamal</td>\n",
       "      <td>*\\n\\nExcept Lakisha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8960</th>\n",
       "      <td>I knew no one who used Mercs, might have been ...</td>\n",
       "      <td>-9</td>\n",
       "      <td>I knew no one who used Emily, might have been ...</td>\n",
       "      <td>I knew no one who used Greg, might have been m...</td>\n",
       "      <td>I knew no one who used Jamal, might have been ...</td>\n",
       "      <td>I knew no one who used Lakisha, might have bee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>96 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  score  \\\n",
       "6593   I'd say we are paying too little for games lik...  127.0   \n",
       "21192  It's mother had been killed so he brought it u...   65.0   \n",
       "4454              Halle Berry's tits, so everybody wins.   92.0   \n",
       "22030  Jamie Foxx has kids too. So does Christoph Wal...     97   \n",
       "1486   Sort of an Ironic thing to say on reddit, no? ...    6.0   \n",
       "...                                                  ...    ...   \n",
       "6051   now your calling something that disagrees with...     -7   \n",
       "28262  I've always found the Dutch to be easy to spot...    133   \n",
       "7496   Fuck Ron Paul?\\n(Wonka Face)\\nPlease, tell me ...    -20   \n",
       "14588  &gt;*Everything sounds better in Latin.*\\n\\nEx...    196   \n",
       "8960   I knew no one who used Mercs, might have been ...     -9   \n",
       "\n",
       "                                              Emily_text  \\\n",
       "6593   I'd say we are paying too little for games lik...   \n",
       "21192          He calls it Emily and I call it Rikimaru.   \n",
       "4454                      Emily tits, so everybody wins.   \n",
       "22030                                Emily has kids too.   \n",
       "1486   People on here always advocating voting the ca...   \n",
       "...                                                  ...   \n",
       "6051   your calling something that disagrees with you...   \n",
       "28262  When in groups they are easy to spot because y...   \n",
       "7496                                       Fuck Emily?\\n   \n",
       "14588                               *\\n\\nExcept Emily...   \n",
       "8960   I knew no one who used Emily, might have been ...   \n",
       "\n",
       "                                               Greg_text  \\\n",
       "6593   I'd say we are paying too little for games lik...   \n",
       "21192           He calls it Greg and I call it Rikimaru.   \n",
       "4454                       Greg tits, so everybody wins.   \n",
       "22030                                 Greg has kids too.   \n",
       "1486   People on here always advocating voting the ca...   \n",
       "...                                                  ...   \n",
       "6051   your calling something that disagrees with you...   \n",
       "28262  When in groups they are easy to spot because y...   \n",
       "7496                                          Fuck Greg?   \n",
       "14588                                *\\n\\nExcept Greg...   \n",
       "8960   I knew no one who used Greg, might have been m...   \n",
       "\n",
       "                                              Jamal_text  \\\n",
       "6593   I'd say we are paying too little for games lik...   \n",
       "21192          He calls it Jamal and I call it Rikimaru.   \n",
       "4454                      Jamal tits, so everybody wins.   \n",
       "22030                                Jamal has kids too.   \n",
       "1486   People on here always advocating voting the ca...   \n",
       "...                                                  ...   \n",
       "6051   your calling something that disagrees with you...   \n",
       "28262  When in groups they are easy to spot because y...   \n",
       "7496                                       Fuck Jamal?\\n   \n",
       "14588                                  *\\n\\nExcept Jamal   \n",
       "8960   I knew no one who used Jamal, might have been ...   \n",
       "\n",
       "                                            Lakisha_text  \n",
       "6593   I'd say we are paying too little for games lik...  \n",
       "21192        He calls it Lakisha and I call it Rikimaru.  \n",
       "4454                    Lakisha tits, so everybody wins.  \n",
       "22030                              Lakisha has kids too.  \n",
       "1486   People on here always advocating voting the ca...  \n",
       "...                                                  ...  \n",
       "6051   your calling something that disagrees with you...  \n",
       "28262  When in groups they are easy to spot because y...  \n",
       "7496                                       Fuck Lakisha?  \n",
       "14588                             *\\n\\nExcept Lakisha...  \n",
       "8960   I knew no one who used Lakisha, might have bee...  \n",
       "\n",
       "[96 rows x 6 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NER_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pronouns replacement\n",
    "pronoun_text = data[data['text_contains_pronoun'] == 1]['text']\n",
    "pronoun_parent_text = data[data['parent_text_contains_pronoun'] == 1]['parent_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Mapping\n",
    "Here are the different scores that we may want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type_map_full = {\n",
    "    \"toxicity_score\": \"TOXICITY\",\n",
    "    \"identity_score\": \"IDENTITY_ATTACK\",\n",
    "    \"insult_score\": \"INSULT\",\n",
    "    \"threat_score\": \"THREAT\",\n",
    "    \"sex_score\": \"SEXUALLY_EXPLICIT\",\n",
    "    \"flirtation_score\": \"FLIRTATION\"\n",
    "}\n",
    "\n",
    "score_type_map_toxicity = {\n",
    "    \"toxicity_score\": \"TOXICITY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner\n",
    "Idea for future me: Pooling api keys to make the script run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def score_row_NER(row, score_type_list, add_delay=False):\n",
    "    if add_delay:\n",
    "        time.sleep(2)\n",
    "    try:\n",
    "        score_type_attribute = {value: {} for value in score_type_list}\n",
    "        analyze_request = {\n",
    "            'comment': { 'text': row['text'] },\n",
    "            'languages': [\"en\"],\n",
    "            'requestedAttributes': score_type_attribute\n",
    "        }\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "    except HttpError as err:\n",
    "        if err.resp.status == 429:\n",
    "            print('Quota limit exceeded')\n",
    "            time.sleep(10)\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    value_list = []\n",
    "    \n",
    "    for score_type in score_type_list:\n",
    "        value_list.append(\n",
    "            response['attributeScores'][score_type]['summaryScore']['value'])\n",
    "\n",
    "    for name in NER_bank:\n",
    "        if add_delay:\n",
    "            time.sleep(2)\n",
    "        try:\n",
    "            score_type_attribute = {value: {} for value in score_type_list}\n",
    "            analyze_request = {\n",
    "                'comment': { 'text': row[name + '_text'] },\n",
    "                'languages': [\"en\"],\n",
    "                'requestedAttributes': score_type_attribute\n",
    "            }\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "\n",
    "        except HttpError as err:\n",
    "            if err.resp.status == 429:\n",
    "                print('Quota limit exceeded')\n",
    "                time.sleep(10)\n",
    "                response = client.comments().analyze(body=analyze_request).execute()\n",
    "            # else:\n",
    "            #     raise\n",
    "\n",
    "        for score_type in score_type_list:\n",
    "            value_list.append(\n",
    "                response['attributeScores'][score_type]['summaryScore']['value'])\n",
    "    \n",
    "    return tuple(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df = NER_df.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Replace score_type_map_toxicity with score_type_map_full to run on all available toxicity scores\n",
    "score_col_names = list(score_type_map_toxicity.keys())\n",
    "score_type_list = list(score_type_map_toxicity.values())\n",
    "\n",
    "results = NER_df.apply(lambda row: score_row_NER(\n",
    "    row, \n",
    "    score_type_list=score_type_list, \n",
    "    add_delay=True), axis=1, result_type='expand')\n",
    "results.columns = ['score','Emily_score','Greg_score','Jamal_score','Lakisha_score']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/ipykernel_30877/20735409.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[name + '_text'] = t\n",
      "/var/folders/qj/gh_j11514m37mqtfrlr885k40000gn/T/ipykernel_30877/20735409.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  row[name + '_text'] = t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I don't normally advocate violence, but these particular type of Muslims? Fucking wipe them off the face of the earth. Do it now. They are nothing but an evil plague intent on barbaric ways of life and destroying anything that doesn't mesh with their ways. They do nothing but cause pain and suffering and hold back others who wish to better themselves. \\n\\nEdit: I can see my comments have polarized this thread. There are a lot of you upset by what I said, and I understand that. What you don't seem to get is that these are not a peaceful people. If they had their way, they'd plunge us all back into the dark ages where men rule women, homosexuals are beheaded, religious nuttery is the only law etc. These are not folks who worship peacefully at your local mosque. These are not rational people. They shoot 14 year old women in the face for seeking education. They destroy history and culture because it contradicts what they believe is true or right. They detonate car bombs in crowded markets maiming and killing innocents. They cause suffering at all turns in the name of a god that does not exist. They are pure evil.\\n\\nDid I write something in a knee jerk fashion? Yep. And I apologize. It was a heated response and I handled it poorly. I don't advocate mass murder of these people. I'll leave my comments in tact to show what an irrational response looks like. Regardless, I'm still livid people like this exist. I took some time to further learn about Salafi Muslims and it appears their leaders forbid the acts of violence they perpetrate. How can you be a leader that does not lead? If your words are ignored, you're not much of a leader IMO. It's for this reason it's hard to know if they really mean to root out the violence in their religion or if it's merely a show to make it seem like they're not all bad. Politics. What does an appropriate response to all this look like? And not just in this case where an artifact was destroyed, but where people are dying for no god-damned reason. It's easy to see the blatant hypocrisy in my original response. I was angry, and I apologize. \""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_NER_examples(data.loc[305393], 'Jamal')['parent_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No. And in honor of the man who said it first and said it best, I call that the \"Ron White Rule\": If you\\'ve seen one set of tits... you pretty much want to see the rest of \\'em.'"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[522]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>Emily_score</th>\n",
       "      <th>Greg_score</th>\n",
       "      <th>Jamal_score</th>\n",
       "      <th>Lakisha_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>0.733072</td>\n",
       "      <td>0.770403</td>\n",
       "      <td>0.811520</td>\n",
       "      <td>0.778266</td>\n",
       "      <td>0.834113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27650</th>\n",
       "      <td>0.116410</td>\n",
       "      <td>0.179565</td>\n",
       "      <td>0.240001</td>\n",
       "      <td>0.247060</td>\n",
       "      <td>0.173781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9847</th>\n",
       "      <td>0.695427</td>\n",
       "      <td>0.093753</td>\n",
       "      <td>0.051529</td>\n",
       "      <td>0.097945</td>\n",
       "      <td>0.037696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>0.717966</td>\n",
       "      <td>0.179710</td>\n",
       "      <td>0.329371</td>\n",
       "      <td>0.327617</td>\n",
       "      <td>0.322458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28428</th>\n",
       "      <td>0.842559</td>\n",
       "      <td>0.091277</td>\n",
       "      <td>0.109158</td>\n",
       "      <td>0.147205</td>\n",
       "      <td>0.084121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>0.888247</td>\n",
       "      <td>0.815041</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.860626</td>\n",
       "      <td>0.860626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20568</th>\n",
       "      <td>0.584254</td>\n",
       "      <td>0.553183</td>\n",
       "      <td>0.605050</td>\n",
       "      <td>0.587656</td>\n",
       "      <td>0.460011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28779</th>\n",
       "      <td>0.141898</td>\n",
       "      <td>0.119585</td>\n",
       "      <td>0.076588</td>\n",
       "      <td>0.110150</td>\n",
       "      <td>0.096796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21512</th>\n",
       "      <td>0.136925</td>\n",
       "      <td>0.108283</td>\n",
       "      <td>0.113671</td>\n",
       "      <td>0.184044</td>\n",
       "      <td>0.092546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8080</th>\n",
       "      <td>0.063378</td>\n",
       "      <td>0.079176</td>\n",
       "      <td>0.055132</td>\n",
       "      <td>0.107483</td>\n",
       "      <td>0.054562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16517</th>\n",
       "      <td>0.072279</td>\n",
       "      <td>0.046233</td>\n",
       "      <td>0.051326</td>\n",
       "      <td>0.074269</td>\n",
       "      <td>0.048796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21409</th>\n",
       "      <td>0.068611</td>\n",
       "      <td>0.053495</td>\n",
       "      <td>0.057920</td>\n",
       "      <td>0.076235</td>\n",
       "      <td>0.052376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6294</th>\n",
       "      <td>0.155379</td>\n",
       "      <td>0.169117</td>\n",
       "      <td>0.203150</td>\n",
       "      <td>0.240466</td>\n",
       "      <td>0.155022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16752</th>\n",
       "      <td>0.494539</td>\n",
       "      <td>0.372523</td>\n",
       "      <td>0.424093</td>\n",
       "      <td>0.409185</td>\n",
       "      <td>0.360529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25481</th>\n",
       "      <td>0.218688</td>\n",
       "      <td>0.197520</td>\n",
       "      <td>0.192346</td>\n",
       "      <td>0.214679</td>\n",
       "      <td>0.177185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26549</th>\n",
       "      <td>0.403093</td>\n",
       "      <td>0.149278</td>\n",
       "      <td>0.157158</td>\n",
       "      <td>0.195524</td>\n",
       "      <td>0.145676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18613</th>\n",
       "      <td>0.063452</td>\n",
       "      <td>0.050581</td>\n",
       "      <td>0.055503</td>\n",
       "      <td>0.067088</td>\n",
       "      <td>0.034923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7934</th>\n",
       "      <td>0.077842</td>\n",
       "      <td>0.080179</td>\n",
       "      <td>0.086363</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.057038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6051</th>\n",
       "      <td>0.278074</td>\n",
       "      <td>0.310894</td>\n",
       "      <td>0.310894</td>\n",
       "      <td>0.355308</td>\n",
       "      <td>0.309578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          score  Emily_score  Greg_score  Jamal_score  Lakisha_score\n",
       "522    0.733072     0.770403    0.811520     0.778266       0.834113\n",
       "27650  0.116410     0.179565    0.240001     0.247060       0.173781\n",
       "9847   0.695427     0.093753    0.051529     0.097945       0.037696\n",
       "7382   0.717966     0.179710    0.329371     0.327617       0.322458\n",
       "28428  0.842559     0.091277    0.109158     0.147205       0.084121\n",
       "765    0.888247     0.815041    0.860626     0.860626       0.860626\n",
       "20568  0.584254     0.553183    0.605050     0.587656       0.460011\n",
       "28779  0.141898     0.119585    0.076588     0.110150       0.096796\n",
       "21512  0.136925     0.108283    0.113671     0.184044       0.092546\n",
       "8080   0.063378     0.079176    0.055132     0.107483       0.054562\n",
       "16517  0.072279     0.046233    0.051326     0.074269       0.048796\n",
       "21409  0.068611     0.053495    0.057920     0.076235       0.052376\n",
       "6294   0.155379     0.169117    0.203150     0.240466       0.155022\n",
       "16752  0.494539     0.372523    0.424093     0.409185       0.360529\n",
       "25481  0.218688     0.197520    0.192346     0.214679       0.177185\n",
       "26549  0.403093     0.149278    0.157158     0.195524       0.145676\n",
       "18613  0.063452     0.050581    0.055503     0.067088       0.034923\n",
       "7934   0.077842     0.080179    0.086363     0.122671       0.057038\n",
       "6051   0.278074     0.310894    0.310894     0.355308       0.309578"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_df.to_csv('toxicity_benchmark_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Calculation\n",
    "Now that everything is in a nice dataframe, we can do some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score            0.355373\n",
       "Emily_score      0.232621\n",
       "Greg_score       0.252179\n",
       "Jamal_score      0.273867\n",
       "Lakisha_score    0.229360\n",
       "dtype: float64"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.astype(np.float64)\n",
    "results.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "score            0.299075\n",
       "Emily_score      0.234397\n",
       "Greg_score       0.253369\n",
       "Jamal_score      0.234584\n",
       "Lakisha_score    0.249908\n",
       "dtype: float64"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4c30e62cc86598b4d0e4d9d3ae67c05bd10d53a2899f1f259fdb6c9b5a74f23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('perspective')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
