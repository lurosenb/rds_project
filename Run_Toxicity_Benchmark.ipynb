{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from googleapiclient.errors import HttpError\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "with open('api_key.txt', 'r') as file:\n",
    "    API_KEY = file.read().rstrip()\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen(seed=56):\n",
    "    df1 = pd.read_csv('comments_negative.csv')\n",
    "    df2 = pd.read_csv('comments_positive.csv')\n",
    "    df_whole = pd.concat([df1,df2])\n",
    "\n",
    "    binary_contains_columns = ['text_contains_NER','parent_text_contains_NER','text_contains_pronoun','parent_text_contains_pronoun']\n",
    "    columns_to_keep = ['text','score','parent_text','parent_score']\n",
    "    all_columns = columns_to_keep + binary_contains_columns \n",
    "    for col in binary_contains_columns:\n",
    "        df_whole[col] = 0\n",
    "\n",
    "    sub_df = df_whole.sample(frac=0.01, random_state=seed)\n",
    "\n",
    "    def check_row_for_NER_and_pronouns(row):\n",
    "        if row['text'].isupper():\n",
    "            return row\n",
    "\n",
    "        parsed = NER(row['text'])\n",
    "        parsed_parent = NER(row['parent_text'])\n",
    "        if len(parsed.ents) > 0:\n",
    "            row['text_contains_NER'] = 1\n",
    "        if len(parsed_parent.ents) > 0:\n",
    "            row['parent_text_contains_NER'] = 1\n",
    "        counter = 0 \n",
    "        for word in parsed:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        counter = 0 \n",
    "        for word in parsed_parent:\n",
    "            if word.pos_ == 'PRON':\n",
    "                counter += 1\n",
    "            if counter > 1:\n",
    "                row['parent_text_contains_pronoun'] = 1\n",
    "                continue\n",
    "        return row\n",
    "\n",
    "    data = sub_df.apply(lambda row: check_row_for_NER_and_pronouns(row), axis=1)\n",
    "\n",
    "    data = data[(data['text_contains_NER'] == 1) | \n",
    "         (data['parent_text_contains_NER'] == 1) | \n",
    "         (data['text_contains_pronoun'] == 1) | \n",
    "         (data['parent_text_contains_pronoun'] == 1)]\n",
    "    data = data[all_columns]\n",
    "\n",
    "    data.to_csv('reddit_comments.csv')\n",
    "    return data\n",
    "\n",
    "# NOTE: Data can only be regend if comments_negative, comments_positive csvs are in directory\n",
    "data = pd.read_csv('reddit_comments.csv')\n",
    "#data = regen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "      <th>parent_text</th>\n",
       "      <th>parent_score</th>\n",
       "      <th>text_contains_NER</th>\n",
       "      <th>parent_text_contains_NER</th>\n",
       "      <th>text_contains_pronoun</th>\n",
       "      <th>parent_text_contains_pronoun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1039091</td>\n",
       "      <td>Then you spend the next few minutes screaming ...</td>\n",
       "      <td>113</td>\n",
       "      <td>The F.E.A.R games. Rounding a corner, hearing ...</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1667861</td>\n",
       "      <td>But the national anthem isn't part of the mili...</td>\n",
       "      <td>76</td>\n",
       "      <td>I was surprised how militarist the USA really ...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1187133</td>\n",
       "      <td>Always was easy  against the computer. Half th...</td>\n",
       "      <td>101</td>\n",
       "      <td>Play the small islands maps.\\n\\nColonise. Ever...</td>\n",
       "      <td>365.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>317042</td>\n",
       "      <td>The best way he can show his wife he loves her...</td>\n",
       "      <td>279</td>\n",
       "      <td>I'm not pregnant, and I'm also a male. My lunc...</td>\n",
       "      <td>376.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1458923</td>\n",
       "      <td>Well the crazy guy went and shot him.</td>\n",
       "      <td>-8</td>\n",
       "      <td>So it's really his fault that a crazy guy atta...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35360</th>\n",
       "      <td>1035898</td>\n",
       "      <td>Yeah, I've worked in childcare for ten years a...</td>\n",
       "      <td>113</td>\n",
       "      <td>Yeah, it's always a bit of a gamble to badmout...</td>\n",
       "      <td>258.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35361</th>\n",
       "      <td>1738081</td>\n",
       "      <td>Huh? I'm not hating or being a fanboy. I'm say...</td>\n",
       "      <td>-7</td>\n",
       "      <td>And here, everyone, is what we call a fanboy. ...</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35362</th>\n",
       "      <td>1332570</td>\n",
       "      <td>Liek dis if u cry everytim</td>\n",
       "      <td>-9</td>\n",
       "      <td>Have you seen the comic that (I believe) origi...</td>\n",
       "      <td>204.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35363</th>\n",
       "      <td>1552465</td>\n",
       "      <td>Congratulations on your baby girl.\\n\\nedit: Wo...</td>\n",
       "      <td>81</td>\n",
       "      <td>I'm adopted and have even less info on my biol...</td>\n",
       "      <td>156.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35364</th>\n",
       "      <td>643828</td>\n",
       "      <td>Anyone else notice that Bruce Willis was actua...</td>\n",
       "      <td>165</td>\n",
       "      <td>nope only you</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35365 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text score  \\\n",
       "0        1039091  Then you spend the next few minutes screaming ...   113   \n",
       "1        1667861  But the national anthem isn't part of the mili...    76   \n",
       "2        1187133  Always was easy  against the computer. Half th...   101   \n",
       "3         317042  The best way he can show his wife he loves her...   279   \n",
       "4        1458923              Well the crazy guy went and shot him.    -8   \n",
       "...          ...                                                ...   ...   \n",
       "35360    1035898  Yeah, I've worked in childcare for ten years a...   113   \n",
       "35361    1738081  Huh? I'm not hating or being a fanboy. I'm say...    -7   \n",
       "35362    1332570                         Liek dis if u cry everytim    -9   \n",
       "35363    1552465  Congratulations on your baby girl.\\n\\nedit: Wo...    81   \n",
       "35364     643828  Anyone else notice that Bruce Willis was actua...   165   \n",
       "\n",
       "                                             parent_text  parent_score  \\\n",
       "0      The F.E.A.R games. Rounding a corner, hearing ...         101.0   \n",
       "1      I was surprised how militarist the USA really ...          15.0   \n",
       "2      Play the small islands maps.\\n\\nColonise. Ever...         365.0   \n",
       "3      I'm not pregnant, and I'm also a male. My lunc...         376.0   \n",
       "4      So it's really his fault that a crazy guy atta...           3.0   \n",
       "...                                                  ...           ...   \n",
       "35360  Yeah, it's always a bit of a gamble to badmout...         258.0   \n",
       "35361  And here, everyone, is what we call a fanboy. ...           8.0   \n",
       "35362  Have you seen the comic that (I believe) origi...         204.0   \n",
       "35363  I'm adopted and have even less info on my biol...         156.0   \n",
       "35364                                      nope only you         237.0   \n",
       "\n",
       "       text_contains_NER  parent_text_contains_NER  text_contains_pronoun  \\\n",
       "0                    1.0                       1.0                    1.0   \n",
       "1                    0.0                       1.0                    0.0   \n",
       "2                    1.0                       1.0                    0.0   \n",
       "3                    1.0                       0.0                    1.0   \n",
       "4                    0.0                       0.0                    0.0   \n",
       "...                  ...                       ...                    ...   \n",
       "35360                1.0                       1.0                    1.0   \n",
       "35361                0.0                       0.0                    1.0   \n",
       "35362                0.0                       1.0                    0.0   \n",
       "35363                1.0                       1.0                    1.0   \n",
       "35364                1.0                       0.0                    0.0   \n",
       "\n",
       "       parent_text_contains_pronoun  \n",
       "0                               1.0  \n",
       "1                               1.0  \n",
       "2                               0.0  \n",
       "3                               1.0  \n",
       "4                               1.0  \n",
       "...                             ...  \n",
       "35360                           1.0  \n",
       "35361                           1.0  \n",
       "35362                           1.0  \n",
       "35363                           1.0  \n",
       "35364                           0.0  \n",
       "\n",
       "[35365 rows x 9 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Reddit CSV as Dataframe\n",
    "Subsampled from https://www.kaggle.com/datasets/ehallmar/reddit-comment-score-prediction (4 mil -> 50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" PUNCT\n",
      "I PRON\n",
      "'m VERB\n",
      "just ADV\n",
      "hesitant ADJ\n",
      "to PART\n",
      "follow VERB\n",
      "a DET\n",
      "weight NOUN\n",
      "loss NOUN\n",
      "guide NOUN\n",
      "where ADV\n",
      "step NOUN\n",
      "1 NUM\n",
      "is VERB\n",
      "' PUNCT\n",
      "get VERB\n",
      "a DET\n",
      "300 NUM\n",
      "pound NOUN\n",
      "tumor NOUN\n",
      "' PUNCT\n",
      ".... PUNCT\n",
      "\" PUNCT\n"
     ]
    }
   ],
   "source": [
    "# TODO: Analysis\n",
    "text1= NER(data['text'][374])\n",
    "for word in text1:\n",
    "    print(word.text, word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Mapping\n",
    "Here are the different scores that we may want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_type_map_full = {\n",
    "    \"toxicity_score\": \"TOXICITY\",\n",
    "    \"identity_score\": \"IDENTITY_ATTACK\",\n",
    "    \"insult_score\": \"INSULT\",\n",
    "    \"threat_score\": \"THREAT\",\n",
    "    \"sex_score\": \"SEXUALLY_EXPLICIT\",\n",
    "    \"flirtation_score\": \"FLIRTATION\"\n",
    "}\n",
    "\n",
    "score_type_map_toxicity = {\n",
    "    \"toxicity_score\": \"TOXICITY\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runner\n",
    "Idea for future me: Pooling api keys to make the script run faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = discovery.build(\n",
    "  \"commentanalyzer\",\n",
    "  \"v1alpha1\",\n",
    "  developerKey=API_KEY,\n",
    "  discoveryServiceUrl=\"https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1\",\n",
    "  static_discovery=False,\n",
    ")\n",
    "\n",
    "def score_row(row, score_type_list, add_delay=False):\n",
    "    if add_delay:\n",
    "        time.sleep(1)\n",
    "    try:\n",
    "        score_type_attribute = {value: {} for value in score_type_list}\n",
    "        analyze_request = {\n",
    "            'comment': { 'text': row['text'] },\n",
    "            'languages': [\"en\"],\n",
    "            'requestedAttributes': score_type_attribute\n",
    "        }\n",
    "        response = client.comments().analyze(body=analyze_request).execute()\n",
    "    except HttpError as err:\n",
    "        if err.resp.status == 429:\n",
    "            print('Quota limit exceeded')\n",
    "            time.sleep(10)\n",
    "            response = client.comments().analyze(body=analyze_request).execute()\n",
    "        else:\n",
    "            raise\n",
    "    value_list = []\n",
    "    for score_type in score_type_list:\n",
    "        value_list.append(\n",
    "            response['attributeScores'][score_type]['summaryScore']['value'])\n",
    "    return tuple(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Replace score_type_map_toxicity with score_type_map_full to run on all available toxicity scores\n",
    "score_col_names = list(score_type_map_full.keys())\n",
    "score_type_list = list(score_type_map_full.values())\n",
    "print(score_type_list)\n",
    "results = data.apply(lambda row: score_row(\n",
    "    row, \n",
    "    score_type_list=score_type_list, \n",
    "    add_delay=True), axis=1, result_type='expand')\n",
    "results.columns = score_col_names\n",
    "data = data.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('toxicity_benchmark_scores.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stats Calculation\n",
    "Now that everything is in a nice dataframe, we can do some stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['category']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby(['category']).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4c30e62cc86598b4d0e4d9d3ae67c05bd10d53a2899f1f259fdb6c9b5a74f23"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('perspective')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
